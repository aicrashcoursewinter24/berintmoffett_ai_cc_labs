{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aicrashcoursewinter24/berintmoffett_ai_cc_labs/blob/lab02/notebooks/lab_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today, we'll play with turning text into numeric vectors (the process of \"vectorization\"), which first requires splitting up the a long string into something closer to a list of words (or characters).\n",
        "\n",
        "This latter process is the process of \"tokenization\": each word/sub-word/character (the atomic unit of text) is called a \"token\"."
      ],
      "metadata": {
        "collapsed": false,
        "id": "94440f0cb80aab7c"
      },
      "id": "94440f0cb80aab7c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start by installing the \"datasets\" python package, giving you access to some helpful utilities in downloading public datasets from HuggingFace and elsewhere."
      ],
      "metadata": {
        "id": "9Mbp7bhFKsKf"
      },
      "id": "9Mbp7bhFKsKf"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets"
      ],
      "metadata": {
        "id": "nkarjQbk--sN"
      },
      "id": "nkarjQbk--sN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are pre-built tokenizer models, which have both code and mappings between tokens and token *ids* - integers which will be feature columns for the text\n",
        "\n",
        "We will first use the BERT model (the original \"transformer\" from the \"[Attention is All You Need](https://arxiv.org/abs/1706.03762)\" paper), in a form which knows how to differentiate between lower and uppercase characters (some tokenizers lowercase everything first).  It's called \"bert-base-uncased\"."
      ],
      "metadata": {
        "id": "6bc7ZzH_K_q3"
      },
      "id": "6bc7ZzH_K_q3"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "3T4RdUce_K0w"
      },
      "id": "3T4RdUce_K0w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note in the output above, you should see a comment about the \"HF_TOKEN\" secret.  There is also a link to HuggingFace, where you can generate your HF Token (see note below about the word \"token\"). To the left part of the Colab screen, there is a \"key\" icon: you can store your HF_TOKEN as a secret there.  Name it HF_TOKEN and give it \"notebook access\" via the toggle.\n",
        "\n",
        "\n",
        "--\n",
        "\n",
        " note on \"token\": there are now two completely unrelated uses of the word \"token\" in this lab:\n",
        "\n",
        "* \"token\": a unit of text like a word or character (or even multi-word phrase) used in text preprocessing\n",
        "* \"HF_TOKEN\": a password-like thing for getting access to HuggingFace"
      ],
      "metadata": {
        "id": "TmsouK9iMHfL"
      },
      "id": "TmsouK9iMHfL"
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.encode(\"Do not meddle in the affairs of wizards\")"
      ],
      "metadata": {
        "id": "RC54OI7KdEFm"
      },
      "id": "RC54OI7KdEFm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write python code to print the textual tokens in sequential order from a string, using the above tokenizer\n",
        "\n",
        "print(tokenizer.convert_ids_to_tokens(encoded))\n"
      ],
      "metadata": {
        "id": "qZ4Pst-mdd-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0f6da1d-9e5c-4502-92eb-62081b069a5c"
      },
      "id": "qZ4Pst-mdd-n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'Do', 'not', 'me', '##ddle', 'in', 'the', 'affairs', 'of', 'wizard', '##s', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded)"
      ],
      "metadata": {
        "id": "bbIbhRAYdOOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4378176c-d662-4e77-d42c-ae7b69c2aeba"
      },
      "id": "bbIbhRAYdOOC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 2091, 1136, 1143, 13002, 1107, 1103, 5707, 1104, 16678, 1116, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
        "print(encoded_input.keys())\n",
        "print(encoded_input['input_ids'])"
      ],
      "metadata": {
        "id": "Ht_0MoRXAX44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba94c73-46b1-443c-b603-0dcf7e2f06c3"
      },
      "id": "Ht_0MoRXAX44",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
            "[101, 2091, 1136, 1143, 13002, 1107, 1103, 5707, 1104, 16678, 1116, 117, 1111, 1152, 1132, 11515, 1105, 3613, 1106, 4470, 119, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install sentence-transformers"
      ],
      "metadata": {
        "id": "W2rBjCvgHzIv"
      },
      "id": "W2rBjCvgHzIv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "    'Sentences are passed as a list of string.',\n",
        "    'The quick brown fox jumps over the lazy dog.']\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "#Print the embeddings\n",
        "for sentence, embedding in zip(sentences, embeddings):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "LZLFiGncH8Sj"
      },
      "id": "LZLFiGncH8Sj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, go ahead and explore with the vector representation (the \"embedding\") of any sentence (or string of text, more generally), looking at the tokenized form, the list of token_id integers, or compute cosine similarities between the embeddings:"
      ],
      "metadata": {
        "id": "Ice7JjeWu_CY"
      },
      "id": "Ice7JjeWu_CY"
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"quick\", \"fast\", \"red\", \"blue\", \"ferari\"]\n",
        "single_word_embeddings = model.encode(words)\n",
        "\n",
        "for word, embed in zip(words, single_word_embeddings):\n",
        "  print(\"word: \", word)\n",
        "  print(\"embed: \", embed[0:10])\n",
        "  print(\"\")\n",
        "\n"
      ],
      "metadata": {
        "id": "S-oThXNtWxFE"
      },
      "id": "S-oThXNtWxFE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: python code to compute the matrix of cosines between all of the pairs of words in the list above.\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Compute the cosine similarity between all pairs of words\n",
        "word_embeddings = model.encode(words)\n",
        "word_similarities = cosine_similarity(word_embeddings)\n",
        "# Print the word similarities\n",
        "print(word_similarities)\n"
      ],
      "metadata": {
        "id": "Q8Ii3VZWXdUD",
        "outputId": "7da45938-5f89-4a56-d783-343d54fe87fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Q8Ii3VZWXdUD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0000001  0.6515874  0.3388258  0.33914232 0.28320336]\n",
            " [0.6515874  1.         0.32009655 0.30601805 0.26345903]\n",
            " [0.3388258  0.32009655 1.         0.72944736 0.26313198]\n",
            " [0.33914232 0.30601805 0.72944736 1.         0.22827557]\n",
            " [0.28320336 0.26345903 0.26313198 0.22827557 0.99999976]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: python code for computing cosine similarity between sentence vector embeddings from the above tokenizer and model\n",
        "\n",
        "from scipy.spatial.distance import cosine\n",
        "for sentence in sentences:\n",
        "    print(\"Sentence:\", sentence)\n",
        "print(\"\")\n",
        "print(\"Cosine similarity between the first two sentences:\", cosine(embeddings[0], embeddings[1]))\n",
        "print(\"Cosine similarity between the second and third sentences:\", cosine(embeddings[1], embeddings[2]))\n",
        "print(\"Cosine similarity between the first and third sentences:\", cosine(embeddings[0], embeddings[2]))\n"
      ],
      "metadata": {
        "id": "oPdkeFdRVBcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6cd29f3-3286-4da6-eee1-219903726c23"
      },
      "id": "oPdkeFdRVBcd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: This framework generates embeddings for each input sentence\n",
            "Sentence: Sentences are passed as a list of string.\n",
            "Sentence: The quick brown fox jumps over the lazy dog.\n",
            "\n",
            "Cosine similarity between the first two sentences: 0.4619206190109253\n",
            "Cosine similarity between the second and third sentences: 0.8964101523160934\n",
            "Cosine similarity between the first and third sentences: 0.8819436207413673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Day 2 Work -----"
      ],
      "metadata": {
        "id": "uEs7w30xu4f9"
      },
      "id": "uEs7w30xu4f9"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gensim nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMNUKYUiyJiw",
        "outputId": "d7bb3afd-531a-4b00-8871-c40b0a4ed06f"
      },
      "id": "iMNUKYUiyJiw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsyjgdIXy3tu",
        "outputId": "fdd17800-a469-4001-81da-6606a457181f"
      },
      "id": "jsyjgdIXy3tu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "sentences = [\n",
        "              \"I never bat an eye.\",\n",
        "              \"A bat flew by.\",\n",
        "              \"I hit it with my bat\",\n",
        "             ]\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "sentence_embeddings = model.encode(sentences)\n",
        "embeddings = [\n",
        "    sentence_embeddings[i][sentences[0].split().index('bat')].reshape(1, -1).flatten()\n",
        "      for i in range(len(sentences))\n",
        "    ]\n",
        "\n",
        "similarity_0_1 = cosine(embeddings[0], embeddings[1])\n",
        "similarity_0_2 = cosine(embeddings[0], embeddings[2])\n",
        "similarity_1_2 = cosine(embeddings[1], embeddings[2])\n",
        "\n",
        "print(f\"Similarity 0 to 1: {similarity_0_1}\")\n",
        "print(f\"Similarity 0 to 2: {similarity_0_2}\")\n",
        "print(f\"Similarity 1 to 2: {similarity_1_2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTpeDPiBu8Lx",
        "outputId": "e0f6b925-dfc0-4b2c-d128-3c43af485a35"
      },
      "id": "BTpeDPiBu8Lx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity 0 to 1: 0\n",
            "Similarity 0 to 2: 1.1920928955078125e-07\n",
            "Similarity 1 to 2: 0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}