{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aicrashcoursewinter24/jakes_labs/blob/adding_cookiecutter_base_install/notebooks/lab_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today, we'll play with turning text into numeric vectors (the process of \"vectorization\"), which first requires splitting up the a long string into something closer to a list of words (or characters).\n",
        "\n",
        "This latter process is the process of \"tokenization\": each word/sub-word/character (the atomic unit of text) is called a \"token\"."
      ],
      "metadata": {
        "collapsed": false,
        "id": "94440f0cb80aab7c"
      },
      "id": "94440f0cb80aab7c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start by installing the \"datasets\" python package, giving you access to some helpful utilities in downloading public datasets from HuggingFace and elsewhere."
      ],
      "metadata": {
        "id": "9Mbp7bhFKsKf"
      },
      "id": "9Mbp7bhFKsKf"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets"
      ],
      "metadata": {
        "id": "nkarjQbk--sN"
      },
      "id": "nkarjQbk--sN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are pre-built tokenizer models, which have both code and mappings between tokens and token *ids* - integers which will be feature columns for the text\n",
        "\n",
        "We will first use the BERT model (the original \"transformer\" from the \"[Attention is All You Need](https://arxiv.org/abs/1706.03762)\" paper), in a form which knows how to differentiate between lower and uppercase characters (some tokenizers lowercase everything first).  It's called \"bert-base-uncased\"."
      ],
      "metadata": {
        "id": "6bc7ZzH_K_q3"
      },
      "id": "6bc7ZzH_K_q3"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "3T4RdUce_K0w"
      },
      "id": "3T4RdUce_K0w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note in the output above, you should see a comment about the \"HF_TOKEN\" secret.  There is also a link to HuggingFace, where you can generate your HF Token (see note below about the word \"token\"). To the left part of the Colab screen, there is a \"key\" icon: you can store your HF_TOKEN as a secret there.  Name it HF_TOKEN and give it \"notebook access\" via the toggle.\n",
        "\n",
        "\n",
        "--\n",
        "\n",
        " note on \"token\": there are now two completely unrelated uses of the word \"token\" in this lab:\n",
        "\n",
        "* \"token\": a unit of text like a word or character (or even multi-word phrase) used in text preprocessing\n",
        "* \"HF_TOKEN\": a password-like thing for getting access to HuggingFace"
      ],
      "metadata": {
        "id": "TmsouK9iMHfL"
      },
      "id": "TmsouK9iMHfL"
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.encode(\"Do not meddle in the affairs of wizards\")"
      ],
      "metadata": {
        "id": "RC54OI7KdEFm"
      },
      "id": "RC54OI7KdEFm",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write python code to print the textual tokens in sequential order from a string, using the above tokenizer\n",
        "\n",
        "print(tokenizer.convert_ids_to_tokens(encoded))\n"
      ],
      "metadata": {
        "id": "qZ4Pst-mdd-n"
      },
      "id": "qZ4Pst-mdd-n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded)"
      ],
      "metadata": {
        "id": "bbIbhRAYdOOC"
      },
      "id": "bbIbhRAYdOOC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
        "print(encoded_input.keys())\n",
        "print(encoded_input['input_ids'])"
      ],
      "metadata": {
        "id": "Ht_0MoRXAX44"
      },
      "id": "Ht_0MoRXAX44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install sentence-transformers"
      ],
      "metadata": {
        "id": "W2rBjCvgHzIv"
      },
      "id": "W2rBjCvgHzIv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "    'Sentences are passed as a list of string.',\n",
        "    'The quick brown fox jumps over the lazy dog.']\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "#Print the embeddings\n",
        "for sentence, embedding in zip(sentences, embeddings):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "LZLFiGncH8Sj"
      },
      "id": "LZLFiGncH8Sj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, go ahead and explore with the vector representation (the \"embedding\") of any sentence (or string of text, more generally), looking at the tokenized form, the list of token_id integers, or compute cosine similarities between the embeddings:"
      ],
      "metadata": {
        "id": "Ice7JjeWu_CY"
      },
      "id": "Ice7JjeWu_CY"
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"quick\", \"fast\", \"red\", \"blue\", \"ferari\"]\n",
        "single_word_embeddings = model.encode(words)\n",
        "\n",
        "for word, embed in zip(words, single_word_embeddings):\n",
        "  print(\"word: \", word)\n",
        "  print(\"embed: \", embed[0:10])\n",
        "  print(\"\")\n",
        "\n"
      ],
      "metadata": {
        "id": "S-oThXNtWxFE"
      },
      "id": "S-oThXNtWxFE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: python code to compute the matrix of cosines between all of the pairs of words in the list above.\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Compute the cosine similarity between all pairs of words\n",
        "word_embeddings = model.encode(words)\n",
        "word_similarities = cosine_similarity(word_embeddings)\n",
        "# Print the word similarities\n",
        "print(word_similarities)\n"
      ],
      "metadata": {
        "id": "Q8Ii3VZWXdUD",
        "outputId": "feaab10b-725f-4c94-e07d-0d1cd0eeb55f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Q8Ii3VZWXdUD",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0000001  0.6515874  0.3388258  0.33914232 0.28320336]\n",
            " [0.6515874  1.         0.32009655 0.30601805 0.26345903]\n",
            " [0.3388258  0.32009655 1.         0.72944736 0.26313198]\n",
            " [0.33914232 0.30601805 0.72944736 1.         0.22827557]\n",
            " [0.28320336 0.26345903 0.26313198 0.22827557 0.99999976]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: python code for computing cosine similarity between sentence vector embeddings from the above tokenizer and model\n",
        "\n",
        "from scipy.spatial.distance import cosine\n",
        "for sentence in sentences:\n",
        "    print(\"Sentence:\", sentence)\n",
        "print(\"\")\n",
        "print(\"Cosine similarity between the first two sentences:\", cosine(embeddings[0], embeddings[1]))\n",
        "print(\"Cosine similarity between the second and third sentences:\", cosine(embeddings[1], embeddings[2]))\n",
        "print(\"Cosine similarity between the first and third sentences:\", cosine(embeddings[0], embeddings[2]))\n"
      ],
      "metadata": {
        "id": "oPdkeFdRVBcd"
      },
      "id": "oPdkeFdRVBcd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

